{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os.path\n",
    "import json\n",
    "import re\n",
    "from bz2 import BZ2File\n",
    "from urllib import request\n",
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "fname = \"cmv.tar.bz2\"\n",
    "f = open(fname, 'rb')\n",
    "\n",
    "\n",
    "tar = tarfile.open(fileobj=f, mode=\"r\")\n",
    "\n",
    "# Extract the file we are interested in\n",
    "\n",
    "train_fname = \"all/train_period_data.jsonlist.bz2\"\n",
    "\n",
    "train_bzlist = tar.extractfile(train_fname)\n",
    "\n",
    "# Deserialize the JSON list\n",
    "data_train = [\n",
    "    json.loads(line.decode('utf-8'))\n",
    "    for line in BZ2File(train_bzlist)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=[]\n",
    "for item in data_train[0:20]:\n",
    "    data1.append(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import array\n",
    "import re\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "no_features = 40000\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=no_features)\n",
    "\n",
    "#Function for tfidf_interact\n",
    "def word_over(strC, strO): \n",
    "    a = set(strC.split()) \n",
    "    b = set(strO.split())\n",
    "    c = a.intersection(b)\n",
    "    f1 = float(len(c))\n",
    "    f2 = float(len(c))/(len(a))\n",
    "    f3 = float(len(c))/(len(b))\n",
    "    f4 = float(len(c))/(len(a) + len(b) - len(c))\n",
    "    return [f1,f2,f3,f4]\n",
    "\n",
    "#Function for clean up using regex\n",
    "def remreg(cmv_post):\n",
    "    xy=re.sub('[^a-zA-Z0-9][(/w)]', '', cmv_post)\n",
    "    return xy\n",
    "\n",
    "#Function for clean up\n",
    "def cleanup(cmv_post):\n",
    "    lines = [line for line in cmv_post.splitlines()\n",
    "             if not line.lstrip().startswith(\"&gt;\")\n",
    "             and not line.lstrip().startswith(\"____\")\n",
    "             and \"edit\" not in \" \".join(line.lower().split()[:2])\n",
    "            ]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "inpdat=[]\n",
    "y=0\n",
    "#Loop that gives each discussion\n",
    "for idx,node in enumerate(data1):\n",
    "    t1=tuple()\n",
    "    op_node=dict()\n",
    "    comments=[]\n",
    "    word4=[]\n",
    "    #Cleaning the opinion post\n",
    "    if data1[idx]['selftext'] == None:\n",
    "        del data1[idx]\n",
    "        break\n",
    "    a = cleanup(data1[idx]['selftext'])\n",
    "    word2 = \"\"\n",
    "    #print(idx)\n",
    "    for word in a.split(' '):\n",
    "        word1 = remreg(word)\n",
    "        word2 = word2+word1+\" \"\n",
    "    for w1 in word2.split():\n",
    "        word4.append(w1)\n",
    "    if len(word4)<100:\n",
    "        del data1[idx]\n",
    "        break\n",
    "    op_node[\"parent_id\"]=data1[idx]['subreddit_id']\n",
    "    for sent in word2.split('.'):\n",
    "        postname=sent\n",
    "        break\n",
    "    op_node[\"post_name\"]=postname\n",
    "    op_node[\"text\"]=word2\n",
    "    \n",
    "    comments = []\n",
    "    word6 = \"\"\n",
    "    #node_tfidf_features = []\n",
    "    x=0\n",
    "    #loop that gives each comment of the discussion\n",
    "    for item in data1[idx]['comments']:\n",
    "        comment=dict()\n",
    "        l1=[]\n",
    "        if 'body' in item.keys():\n",
    "            c1 = cleanup(item['body'])\n",
    "            for word in c1.split(' '):\n",
    "                word5 = remreg(word)\n",
    "                word6 = word6+word5+\" \"\n",
    "            if word6 == \" \":\n",
    "                break\n",
    "            if word6 == \"[deleted] \":\n",
    "                break\n",
    "            l1.append(word6)\n",
    "            tfidf = tfidf_vectorizer.fit_transform(l1)\n",
    "            comment[\"node_tfidf\"]=tfidf\n",
    "            #tfidf_feature_names = [tfidf_vectorizer.get_feature_names()]\n",
    "            #node_tfidf_features.append(tfidf_feature_names)\n",
    "            comment[\"tfidf_interact\"]=word_over(word6,word2)\n",
    "            if item['author_flair_text']:\n",
    "                comment[\"delta\"]=1\n",
    "            else: comment[\"delta\"]=0\n",
    "            comment[\"text\"]=word6\n",
    "            comments.append(comment)\n",
    "            word6=\"\"\n",
    "            word5=\"\"\n",
    "            x=x+1\n",
    "    t1=(op_node,comments) #Main tuple for each discussion\n",
    "    inpdat.append(t1)     #inpdat is supposed to be the json pickled file for input\n",
    "y=y+1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
